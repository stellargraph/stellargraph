{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of reddit user posts using graph machine learning with Stellargraph\n",
    "\n",
    "We apply the graph machine algorithm APPNP [1] to the task of classifying reddit user posts into 41 different categories using the dataset published in [2] which can be downloaded [here](http://snap.stanford.edu/graphsage/reddit.zip).  \n",
    "\n",
    "The following is a description of the dataset [2]:\n",
    "\n",
    ">Reddit is a large online discussion forum where users post and comment on content in different topical\n",
    ">communities. We constructed a graph dataset from Reddit posts made in the month of September, 2014. The node label in this case is the community, or “subreddit”, that a post belongs to. We sampled\n",
    ">50 large communities and built a post-to-post graph, connecting posts if the same user comments\n",
    ">on both. In total this dataset contains 232,965 posts with an average degree of 492. We use the first\n",
    ">20 days for training and the remaining days for testing (with 30% used for validation). For features,\n",
    ">we use off-the-shelf 300-dimensional GloVe CommonCrawl word vectors [3]; for each post, we\n",
    ">concatenated (i) the average embedding of the post title, (ii) the average embedding of all the post’s\n",
    ">comments (iii) the post’s score, and (iv) the number of comments made on the post.\n",
    "\n",
    "\n",
    "We demonstrate the advantage of using graph features and the scalability of the APPNP algorithm for node classification on the reddit dataset.  We first train a MLP on the node features and then propagate this model using APPNP.  Training is only done on the node features, this approach allows model training to be completed in under a 1 minute on an 8th gen quad-core i7.\n",
    "\n",
    "\n",
    "**References**\n",
    "\n",
    "1. Predict then propagate: Graph neural networks meet personalized pagerank. J. Klicpera,  A. Bojchevski, & S. Günnemann arxiv:1810.05997, 2018.\n",
    "\n",
    "\n",
    "2. Inductive Representation Learning on Large Graphs. W.L. Hamilton, R. Ying, and J. Leskovec arXiv:1706.02216 [cs.SI], 2017.\n",
    "\n",
    "\n",
    "3. Glove: Global vectors for word representation. J. Pennington, R. Socher, and C. D. Manning. In EMNLP, 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import stellargraph as sg\n",
    "from stellargraph.mapper import FullBatchNodeGenerator\n",
    "from stellargraph.layer import APPNP\n",
    "from stellargraph.layer.appnp import APPNPPropagationLayer\n",
    "from stellargraph.core.utils import GCN_Aadj_feats_op\n",
    "\n",
    "from tensorflow.keras import layers, optimizers, losses, metrics, Model, models\n",
    "from sklearn import preprocessing, feature_extraction\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Reddit Dataset\n",
    "\n",
    "First, we load the reddit dataset which is stored as a series of json files.  We first load the graph data and then the node features and labels and ensure that indexing is consistent across the graph, labels, and node features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.expanduser(\"~/data/reddit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reddit(data_dir):\n",
    "\n",
    "    with open(os.path.join(data_dir, \"reddit-G.json\")) as gfile:\n",
    "        graph_data = json.load(gfile)\n",
    "\n",
    "    list_node_ids = list(d[\"id\"] for d in graph_data[\"nodes\"])\n",
    "\n",
    "    edge_generator = ((link[\"source\"], link[\"target\"]) for link in graph_data[\"links\"])\n",
    "    edge_df = pd.DataFrame(edge_generator, columns=[\"target\", \"source\"])\n",
    "\n",
    "    with open(os.path.join(data_dir, \"reddit-class_map.json\")) as tfile:\n",
    "        labels = json.load(tfile)\n",
    "\n",
    "    feats = np.load(data_dir + \"/reddit-feats.npy\")\n",
    "\n",
    "    feats[:, 0] = np.log(feats[:, 0] + 1.0)\n",
    "    feats[:, 1] = np.log(feats[:, 1] - min(np.min(feats[:, 1]), -1))\n",
    "\n",
    "    feat_id_map = json.load(open(data_dir + \"/reddit-id_map.json\"))\n",
    "\n",
    "    # sort node features to match the order of feat_id_map\n",
    "    sorted_idxs = np.array([feat_id_map[key] for key in list_node_ids])\n",
    "    feats = feats[sorted_idxs, :]\n",
    "    node_data = pd.DataFrame(feats)\n",
    "\n",
    "    # sort node labnels to match the order of feat_id_map\n",
    "    labels = np.array([labels[key] for key in list_node_ids])\n",
    "\n",
    "    return edge_df, node_data, labels, list_node_ids, graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 231443\n",
      "Number of edges: 11606919\n"
     ]
    }
   ],
   "source": [
    "edge_df, node_data, labels, list_node_ids, graph_data = load_reddit(data_dir)\n",
    "print(\"Number of nodes:\", len(node_data))\n",
    "print(\"Number of edges:\", len(edge_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoding = preprocessing.OneHotEncoder(sparse=False, categories=\"auto\")\n",
    "targets = target_encoding.fit_transform(labels.reshape(-1, 1))\n",
    "targets = pd.DataFrame(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then split the data into train/val/test based on the labels stored in the dataset. This is the same train/val/test split used in the graphsage paper. Then we fit a standard scaler on only the training data and use it to standardize all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152410 training nodes.\n",
      "23699 validation nodes.\n",
      "55334 testing nodes.\n"
     ]
    }
   ],
   "source": [
    "def map_node_to_split(node):\n",
    "    if node[\"test\"]:\n",
    "        return \"test\"\n",
    "    elif node[\"val\"]:\n",
    "        return \"val\"\n",
    "    else:\n",
    "        return \"train\"\n",
    "\n",
    "\n",
    "train_test_val_dict = dict(\n",
    "    zip(list_node_ids, map(map_node_to_split, graph_data[\"nodes\"]))\n",
    ")\n",
    "\n",
    "train_mask = [(train_test_val_dict[key] == \"train\") for key in list_node_ids]\n",
    "val_mask = [(train_test_val_dict[key] == \"val\") for key in list_node_ids]\n",
    "test_mask = [(train_test_val_dict[key] == \"test\") for key in list_node_ids]\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(node_data[train_mask].values)\n",
    "node_data.iloc[:, :] = scaler.transform(node_data.values)\n",
    "\n",
    "train_data, train_targets = node_data[train_mask], targets[train_mask]\n",
    "val_data, val_targets = node_data[val_mask], targets[val_mask]\n",
    "test_data, test_targets = node_data[test_mask], targets[test_mask]\n",
    "\n",
    "print(\"{} training nodes.\".format(len(train_data)))\n",
    "print(\"{} validation nodes.\".format(len(val_data)))\n",
    "print(\"{} testing nodes.\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an adjacnecy matrix and StellarGraph generator\n",
    "\n",
    "We know create a `StellarGraph` object and use this to create the data generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = sg.StellarGraph(\n",
    "    nodes=node_data, edges=edge_df, source_column=\"source\", target_column=\"target\"\n",
    ")\n",
    "generator = FullBatchNodeGenerator(G, method=\"gcn\", sparse=True)\n",
    "\n",
    "test_gen = generator.flow(test_data.index, test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We now create a MLP and train on the node features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_layer = layers.Input(shape=(train_data.shape[-1]))\n",
    "\n",
    "layer = layers.Dense(512, activation=\"relu\", kernel_regularizer=\"l2\")(in_layer)\n",
    "layer = layers.Dropout(0.5)(layer)\n",
    "layer = layers.Dense(512, activation=\"relu\", kernel_regularizer=\"l2\")(in_layer)\n",
    "layer = layers.Dropout(0.5)(layer)\n",
    "\n",
    "# note the dimension of the output should equal the number of classes to predict!\n",
    "layer = layers.Dense(train_targets.shape[-1], activation=\"softmax\")(layer)\n",
    "\n",
    "fully_connected_model = Model(inputs=in_layer, outputs=layer)\n",
    "\n",
    "fully_connected_model.compile(\n",
    "    optimizer=optimizers.Adam(lr=0.0001),\n",
    "    loss=losses.categorical_crossentropy,\n",
    "    metrics=[\"acc\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = fully_connected_model.fit(\n",
    "    train_data,\n",
    "    train_targets,\n",
    "    epochs=20,\n",
    "    validation_data=(val_data, val_targets),\n",
    "    batch_size=300,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLP attains an accuracy of ~70% on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55334/55334 [==============================] - 2s 41us/sample - loss: 1.2874 - acc: 0.6991\n",
      "\n",
      "Test Set Metrics:\n",
      "\tloss: 1.2874\n",
      "\tacc: 0.6991\n"
     ]
    }
   ],
   "source": [
    "test_metrics = fully_connected_model.evaluate(test_data, test_targets)\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for name, val in zip(fully_connected_model.metrics_names, test_metrics):\n",
    "    print(\"\\t{}: {:0.4f}\".format(name, val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPNP Propagation\n",
    "\n",
    "We now create an APPNP model and propagate the MLP. No further training is happening in this step.  We then test the propagated model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "appnp = APPNP(\n",
    "    layer_sizes=[train_targets.shape[-1]],\n",
    "    activations=[\"relu\"],\n",
    "    bias=True,\n",
    "    generator=generator,\n",
    "    teleport_probability=0.2,\n",
    "    dropout=0.5,\n",
    "    kernel_regularizer=\"l2\",\n",
    ")\n",
    "\n",
    "x_inp, x_out = appnp.propagate_model(fully_connected_model)\n",
    "predictions = layers.Softmax()(x_out)\n",
    "\n",
    "propagated_model = Model(inputs=x_inp, outputs=predictions)\n",
    "propagated_model.compile(\n",
    "    loss=\"categorical_crossentropy\", metrics=[\"acc\"], optimizer=\"Adam\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Metrics:\n",
      "\tloss: 3.5578\n",
      "\tacc: 0.8662\n"
     ]
    }
   ],
   "source": [
    "test_metrics = propagated_model.evaluate_generator(test_gen)\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for name, val in zip(propagated_model.metrics_names, test_metrics):\n",
    "    print(\"\\t{}: {:0.4f}\".format(name, val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propagating the MLP with APPNP increases the test set accuracy by ~15% without any further training. As we are performing single-label multiclass classification the accuracy is equivalent to the micro F1 metric. This micro F1 is comparable to that attained in the GraphSAGE paper [2]. GraphSAGE with LSTM aggregation attains a best supervised F1 of 0.95 in [2], however APPNP only required ~3 minutes of training on an 8th gen i7 compared to the hours required for GraphSAGE while still attaining a similar F1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
